{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ee183a6",
   "metadata": {},
   "source": [
    "# Adaptive RAG\n",
    "\n",
    "Adaptive RAG is a strategy for RAG that unites query analysis with self-corrective RAG.\n",
    "\n",
    "In the paper, they report query analysis to route across:\n",
    "\n",
    " - No Retrieval\n",
    " - Single-shot RAG\n",
    " - Iterative RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2584c6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\IdeaPad\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-04-12-diffusion-video/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=200, chunk_overlap=50\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "embd = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "vectorstore = InMemoryVectorStore.from_documents(\n",
    "    documents=doc_splits, embedding=embd\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286a7a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grade='1' reasoning=\"This question requires a single retrieval to find the Bears' draft pick.\"\n",
      "grade='1' reasoning='The question asks for a list of types, which can be answered with a single retrieval.'\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "class GradeQuery(BaseModel):\n",
    "    \"\"\"Grade a user query for difficulty. The grade will range from 0-5. \"\"\"\n",
    "\n",
    "    grade: Literal[\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question grade the difficulty for a RAG system.\",\n",
    "    )\n",
    "    reasoning: str = Field(\n",
    "        ...,\n",
    "        description=\"Provide a reasoning for the grade you assigned to the user question.\",\n",
    "    )\n",
    "\n",
    "# LLM with function call\n",
    "llm = init_chat_model(\"gemini-2.0-flash-lite\", temperature=0, model_provider=\"google_genai\")\n",
    "structured_llm_router = llm.with_structured_output(GradeQuery)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are an expert at grading the difficulty of a user question for a RAG system. Your grade will range from 0-5. Grade 0 means no retrieval is needed. Grade 1 is a question that can be answered with a single shot retrieval from a vectorstore, while grade 5 is a question that might require up to 5 retrievals querying a vectorstore with each retrieval being a follow-up to the previous one.\"\"\"\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Rate the difficulty of the following question: \\n\\n {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_router = route_prompt | structured_llm_router\n",
    "print(\n",
    "    question_router.invoke(\n",
    "        {\"question\": \"Who will the Bears draft first in the NFL draft?\"}\n",
    "    )\n",
    ")\n",
    "print(question_router.invoke({\"question\": \"What are the types of agent memory?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db1dbf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
